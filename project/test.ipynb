{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.exposure import histogram\n",
    "from matplotlib.pyplot import bar\n",
    "from skimage.color import rgb2gray,rgb2hsv\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.signal import fftconvolve\n",
    "from scipy import fftpack\n",
    "from skimage.filters import median\n",
    "from skimage.feature import canny\n",
    "from skimage.filters import sobel_h, sobel, sobel_v,roberts, prewitt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from skimage import color\n",
    "import pickle\n",
    "\n",
    "def unsharp_masking(image, blur_radius=5, sharpen_amount=1.0):\n",
    "    \"\"\"Apply unsharp masking followed by dilation to enhance image details.\"\"\"\n",
    "    # Apply Gaussian blur\n",
    "    blurred = cv2.GaussianBlur(image, (0, 0), blur_radius)\n",
    "    \n",
    "    # Apply unsharp masking\n",
    "    sharpened = cv2.addWeighted(image, 1.0 + sharpen_amount, blurred, -sharpen_amount, 0)\n",
    "    \n",
    "    # Ensure the sharpened image doesn't have values below the original image\n",
    "    sharpened = np.where(image >= blurred, sharpened, image)\n",
    "    \n",
    "    return sharpened\n",
    "\n",
    "def remove_noise22(image, dilation_kernel=None):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    bilateral_filtered = cv2.bilateralFilter(gray, 15, sigmaColor=3, sigmaSpace=10)\n",
    "    \n",
    "    median_blurred = cv2.medianBlur(bilateral_filtered, 3)\n",
    "    result = unsharp_masking(median_blurred, blur_radius=10, sharpen_amount=4) * 255\n",
    "    \n",
    "    result = np.uint8(result)\n",
    "    edges = cv2.Canny(result, 80, 255,apertureSize=3)\n",
    "       # Optionally apply dilation\n",
    "    if dilation_kernel is not None:\n",
    "        # Perform dilation\n",
    "        edges = cv2.dilate(edges, dilation_kernel)\n",
    "    return edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tests']\n",
      "Processed and saved: fonts_processed-dataset\\tests\\0.jpeg\n",
      "Processed and saved: fonts_processed-dataset\\tests\\1.jpeg\n",
      "Processed and saved: fonts_processed-dataset\\tests\\16.jpeg\n",
      "Processed and saved: fonts_processed-dataset\\tests\\2.jpeg\n",
      "Processed and saved: fonts_processed-dataset\\tests\\4.jpeg\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def construct_dataset(input_folder, output_folder):\n",
    "    # Iterate over subfolders in the input folder\n",
    "\n",
    "    for root, dirs, files in os.walk(input_folder):\n",
    "        for dir_name in dirs:\n",
    "         \n",
    "            input_subfolder = os.path.join(root, dir_name)\n",
    "            output_subfolder = os.path.join(output_folder, os.path.relpath(input_subfolder, input_folder))\n",
    "            \n",
    "            # Create corresponding subfolder in the output folder\n",
    "            os.makedirs(output_subfolder, exist_ok=True)\n",
    "            \n",
    "            # Iterate over files in the subfolder\n",
    "            for file_name in os.listdir(input_subfolder):\n",
    "                input_image_path = os.path.join(input_subfolder, file_name)\n",
    "                output_image_path = os.path.join(output_subfolder, file_name)\n",
    "                \n",
    "                # Read the image\n",
    "                image = io.imread(input_image_path)\n",
    "                \n",
    "                # Apply preprocessing\n",
    "                # np.ones((kernel_size, kernel_size), np.uint8)\n",
    "                processed_image = remove_noise22(image,dilation_kernel=None)\n",
    "                \n",
    "                # Save the processed image\n",
    "                cv2.imwrite(output_image_path, processed_image)\n",
    "                print(f\"Processed and saved: {output_image_path}\")\n",
    "\n",
    "\n",
    "input_folder = \"fonts-dataset\"\n",
    "output_folder = \"fonts_processed-dataset\"\n",
    "construct_dataset(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSIFTFeatures(input_folder):\n",
    "    sift = cv2.SIFT_create()\n",
    "    SIFTListLabeled = []\n",
    "\n",
    "    # Iterate over subfolders in the input folder\n",
    "    for root, dirs, files in os.walk(input_folder):\n",
    "        for dir_name in dirs:\n",
    "            input_subfolder = os.path.join(root, dir_name)\n",
    "            label = dir_name  # Assuming folder name is the label\n",
    "            \n",
    "            # Get the list of files in the subfolder and sort them based on filenames\n",
    "            file_list = sorted(os.listdir(input_subfolder), key=lambda x: int(x.split('.')[0]))\n",
    "            \n",
    "            # Iterate over sorted files in the subfolder\n",
    "            for file_name in file_list:\n",
    "                input_image_path = os.path.join(input_subfolder, file_name)\n",
    "                image = cv2.imread(input_image_path)\n",
    "                # Convert the image to grayscale\n",
    "                # Detect keypoints and compute descriptors\n",
    "                keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "                # Append descriptors and label as a pair\n",
    "                SIFTListLabeled.append((descriptors, label))\n",
    "\n",
    "    return SIFTListLabeled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training set: 4\n",
      "Number of samples in validation set: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# getSIFTdescriptors\n",
    "input_folder = \"fonts_processed-dataset\"\n",
    "SIFTListLabeled = getSIFTFeatures(input_folder)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, val_data = train_test_split(SIFTListLabeled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the sizes of the training and validation sets\n",
    "print(\"Number of samples in training set:\", len(train_data))\n",
    "print(\"Number of samples in validation set:\", len(val_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans model saved as kmeans_model.pkl\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "#Visual Vocabulary Construction\n",
    "# Concatenate all SIFT descriptors from the training set into a single array\n",
    "all_descriptors_train = np.concatenate([data[0] for data in train_data if data[0] is not None], axis=0)\n",
    "\n",
    "# Perform k-means clustering on the concatenated descriptors\n",
    "k = 70 # Number of clusters\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(all_descriptors_train)\n",
    "\n",
    "# Get the cluster centroids (visual words)\n",
    "visual_words = kmeans.cluster_centers_\n",
    "\n",
    "# Save the trained KMeans model as a pickle file\n",
    "kmeans_model_filename = \"kmeans_model.pkl\"\n",
    "with open(kmeans_model_filename, 'wb') as file:\n",
    "    pickle.dump(kmeans, file)\n",
    "\n",
    "print(\"KMeans model saved as\", kmeans_model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Encoding\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin_min\n",
    "\n",
    "# Initialize empty lists to store histograms for training and validation sets\n",
    "train_histograms = []\n",
    "val_histograms = []\n",
    "\n",
    "# Iterate over training data\n",
    "for descriptors, label in train_data:\n",
    "    if descriptors is not None:  \n",
    "        descriptors = np.array(descriptors)  # Convert to numpy array\n",
    "        # Check if descriptors array is non-empty and has the correct shape\n",
    "        if descriptors.shape[0] > 0 and descriptors.shape[1] > 0:\n",
    "            # Find the nearest visual word for each descriptor\n",
    "            nearest_clusters = pairwise_distances_argmin_min(descriptors, visual_words)[0]\n",
    "            # Count the occurrences of each visual word and create a histogram\n",
    "            histogram, _ = np.histogram(nearest_clusters, bins=np.arange(k+1))\n",
    "            # Append histogram and label to the list\n",
    "            train_histograms.append((histogram, label))\n",
    "\n",
    "# Iterate over validation data\n",
    "for descriptors, label in val_data:\n",
    "    if descriptors is not None:  \n",
    "        descriptors = np.array(descriptors)  # Convert to numpy array\n",
    "        # Check if descriptors array is non-empty and has the correct shape\n",
    "        if descriptors.shape[0] > 0 and descriptors.shape[1] > 0:\n",
    "            # Find the nearest visual word for each descriptor\n",
    "            nearest_clusters = pairwise_distances_argmin_min(descriptors, visual_words)[0]\n",
    "            # Count the occurrences of each visual word and create a histogram\n",
    "            histogram, _ = np.histogram(nearest_clusters, bins=np.arange(k+1))\n",
    "            # Append histogram and label to the list\n",
    "            val_histograms.append((histogram, label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8125\n",
      "Model saved as svm_model.pkl\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Extract features (histograms) and labels from training set\n",
    "X_train = [histogram for histogram, _ in train_histograms]\n",
    "y_train = [label for _, label in train_histograms]\n",
    "\n",
    "# Extract features (histograms) and labels from validation set\n",
    "X_val = [histogram for histogram, _ in val_histograms]\n",
    "y_val = [label for _, label in val_histograms]\n",
    "\n",
    "# Initialize and train SVM classifier\n",
    "svm_classifier = SVC(kernel='linear', random_state=42)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for validation set\n",
    "y_pred = svm_classifier.predict(X_val)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Validation accuracy:\", accuracy)\n",
    "\n",
    "# Save the trained model as a pickle file\n",
    "model_filename = \"svm_model.pkl\"\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(svm_classifier, file)\n",
    "\n",
    "print(\"Model saved as\", model_filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
